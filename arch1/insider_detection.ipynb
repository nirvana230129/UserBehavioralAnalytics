{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "JoegpRvkKOXX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "GeKeeBFLIQkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier"
      ],
      "metadata": {
        "id": "yQKm3gfjMC-Q"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BaseModel"
      ],
      "metadata": {
        "id": "ZCRchyetIXaU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rtwY96a8jWSA"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class AnomalyDetector(BaseEstimator, ABC):\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.threshold = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Обучение модели\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, X):\n",
        "        \"\"\"Предсказание аномальности\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Вероятность аномальности\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrequencyDetector"
      ],
      "metadata": {
        "id": "ouR9HaZjIaJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataFrequencyDetector(AnomalyDetector):\n",
        "    def __init__(self, window_size=3600, contamination=0.1):\n",
        "        super().__init__()\n",
        "        self.model = IsolationForest(contamination=contamination, random_state=42)\n",
        "        self.scaler = StandardScaler()\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def _prepare_features(self, X):\n",
        "        \"\"\"Подготовка признаков для анализа частоты обращений к данным\"\"\"\n",
        "        features = np.column_stack([\n",
        "            X['avg_http_requests_per_day'],\n",
        "            X['avg_emails_sent_per_day'],\n",
        "            X['avg_emails_received_per_day'],\n",
        "            X['avg_file_copies_per_day']\n",
        "        ])\n",
        "        if not self.is_fitted:\n",
        "            self.is_fitted = True\n",
        "            return self.scaler.fit_transform(features)\n",
        "        return self.scaler.transform(features)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        features = self._prepare_features(X)\n",
        "        self.model.fit(features)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        features = self._prepare_features(X)\n",
        "        return self.model.predict(features)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        features = self._prepare_features(X)\n",
        "        scores = -self.model.score_samples(features)\n",
        "        return scores / np.max(scores) if len(scores) > 0 else scores"
      ],
      "metadata": {
        "id": "Js3VGH1RIF9p"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataVolumeDetector"
      ],
      "metadata": {
        "id": "YrNeiqcjIhkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataVolumeDetector(AnomalyDetector):\n",
        "    def __init__(self, window_size=3600, contamination=0.1):\n",
        "        super().__init__()\n",
        "        self.model = IsolationForest(contamination=contamination, random_state=42)\n",
        "        self.scaler = StandardScaler()\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def _prepare_features(self, X):\n",
        "        \"\"\"Подготовка признаков для анализа объема данных\"\"\"\n",
        "        features = np.column_stack([\n",
        "            X['unique_domains'],\n",
        "            X['unique_email_contacts'],\n",
        "            X['unique_file_types'],\n",
        "            X['unique_pcs']\n",
        "        ])\n",
        "        if not self.is_fitted:\n",
        "            self.is_fitted = True\n",
        "            return self.scaler.fit_transform(features)\n",
        "        return self.scaler.transform(features)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        features = self._prepare_features(X)\n",
        "        self.model.fit(features)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        features = self._prepare_features(X)\n",
        "        return self.model.predict(features)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        features = self._prepare_features(X)\n",
        "        scores = -self.model.score_samples(features)\n",
        "        return scores / np.max(scores) if len(scores) > 0 else scores"
      ],
      "metadata": {
        "id": "6YF2fXJTIMKY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FileActivityDetector"
      ],
      "metadata": {
        "id": "txrv4kJFImyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FileActivityDetector(AnomalyDetector):\n",
        "    def __init__(self, contamination=0.1):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        contamination : float, default=0.1\n",
        "            Ожидаемая доля аномалий в данных\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = IsolationForest(\n",
        "            contamination=contamination,\n",
        "            random_state=42,\n",
        "            n_estimators=100\n",
        "        )\n",
        "\n",
        "    def extract_features(self, X):\n",
        "        \"\"\"\n",
        "        Извлечение признаков для анализа файловых операций\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pandas.DataFrame\n",
        "            Датафрейм с признаками пользователей\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            Матрица признаков для анализа файловых операций\n",
        "        \"\"\"\n",
        "        features = []\n",
        "\n",
        "        # Основные признаки\n",
        "        if 'avg_file_copies_per_day' in X.columns:\n",
        "            features.append(X['avg_file_copies_per_day'].values.reshape(-1, 1))\n",
        "\n",
        "        if 'unique_file_types' in X.columns:\n",
        "            features.append(X['unique_file_types'].values.reshape(-1, 1))\n",
        "\n",
        "        # Комбинированные признаки с другими активностями\n",
        "        if 'after_hours_logon_ratio' in X.columns and 'avg_file_copies_per_day' in X.columns:\n",
        "            features.append((X['after_hours_logon_ratio'] * X['avg_file_copies_per_day']).values.reshape(-1, 1))\n",
        "\n",
        "        if 'weekend_logon_ratio' in X.columns and 'avg_file_copies_per_day' in X.columns:\n",
        "            features.append((X['weekend_logon_ratio'] * X['avg_file_copies_per_day']).values.reshape(-1, 1))\n",
        "\n",
        "        if 'device_usage_ratio' in X.columns and 'avg_file_copies_per_day' in X.columns:\n",
        "            features.append((X['device_usage_ratio'] * X['avg_file_copies_per_day']).values.reshape(-1, 1))\n",
        "\n",
        "        # Если нет признаков файловой активности, возвращаем нулевую матрицу\n",
        "        if not features:\n",
        "            return np.zeros((len(X), 1))\n",
        "\n",
        "        return np.hstack(features)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Обучение модели\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pandas.DataFrame\n",
        "            Датафрейм с признаками пользователей\n",
        "        y : array-like, default=None\n",
        "            Игнорируется, добавлен для совместимости\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        self : object\n",
        "            Возвращает себя\n",
        "        \"\"\"\n",
        "        features = self.extract_features(X)\n",
        "        self.model.fit(features)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Предсказание аномальности\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pandas.DataFrame\n",
        "            Датафрейм с признаками пользователей\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            Массив меток: 1 - нормальное поведение, -1 - аномальное\n",
        "        \"\"\"\n",
        "        features = self.extract_features(X)\n",
        "        return self.model.predict(features)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Вероятность аномальности\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pandas.DataFrame\n",
        "            Датафрейм с признаками пользователей\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            Массив вероятностей аномальности\n",
        "        \"\"\"\n",
        "        features = self.extract_features(X)\n",
        "        # Преобразуем decision_function в вероятности\n",
        "        scores = self.model.decision_function(features)\n",
        "        # Нормализуем scores в диапазон [0, 1], где 1 - наиболее аномальное\n",
        "        probs = 1 - (scores - scores.min()) / (scores.max() - scores.min())\n",
        "        return probs\n",
        "\n",
        "    def _prepare_features(self, X):\n",
        "        \"\"\"Подготовка признаков для анализа файловых операций\"\"\"\n",
        "        features = np.column_stack([\n",
        "            X['avg_file_copies_per_day'],\n",
        "            X['unique_file_types'],\n",
        "            X['after_hours_logon_ratio'],\n",
        "            X['weekend_logon_ratio']\n",
        "        ])\n",
        "        return features"
      ],
      "metadata": {
        "id": "iZsTRpxQIplf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GeoLocationDetector"
      ],
      "metadata": {
        "id": "TadLo2UeIze-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geoip2"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFN_rpyEJHsH",
        "outputId": "f0d8487b-d8f9-4cdb-9c87-f48b2f33041e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geoip2\n",
            "  Downloading geoip2-5.1.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from geoip2) (3.11.15)\n",
            "Collecting maxminddb<3.0.0,>=2.7.0 (from geoip2)\n",
            "  Downloading maxminddb-2.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.11/dist-packages (from geoip2) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->geoip2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->geoip2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->geoip2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->geoip2) (2025.4.26)\n",
            "Downloading geoip2-5.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading maxminddb-2.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: maxminddb, geoip2\n",
            "Successfully installed geoip2-5.1.0 maxminddb-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import geoip2.database\n",
        "\n",
        "class GeoLocationDetector(AnomalyDetector):\n",
        "    def __init__(self, geoip_db_path, known_good_ips=None, contamination=0.1):\n",
        "        super().__init__()\n",
        "        self.model = IsolationForest(contamination=contamination, random_state=42)\n",
        "        self.geoip_reader = geoip2.database.Reader(geoip_db_path)\n",
        "        self.known_good_ips = known_good_ips or set()\n",
        "\n",
        "    def _get_location_features(self, ip):\n",
        "        \"\"\"Получение географических признаков по IP\"\"\"\n",
        "        try:\n",
        "            response = self.geoip_reader.city(ip)\n",
        "            return [\n",
        "                response.location.latitude,\n",
        "                response.location.longitude,\n",
        "                int(ip in self.known_good_ips)\n",
        "            ]\n",
        "        except:\n",
        "            return [0, 0, 0]  # дефолтные значения при ошибке\n",
        "\n",
        "    def _prepare_features(self, X):\n",
        "        \"\"\"Подготовка географических признаков\"\"\"\n",
        "        features = []\n",
        "        for ip in X['ip_address']:\n",
        "            features.append(self._get_location_features(ip))\n",
        "        return np.array(features)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        features = self._prepare_features(X)\n",
        "        self.model.fit(features)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        features = self._prepare_features(X)\n",
        "        return self.model.predict(features)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        features = self._prepare_features(X)\n",
        "        scores = -self.model.score_samples(features)\n",
        "        return scores / np.max(scores)\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Закрываем reader при удалении объекта\"\"\"\n",
        "        if hasattr(self, 'geoip_reader'):\n",
        "            self.geoip_reader.close()"
      ],
      "metadata": {
        "id": "cgEDHDGZIyAY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResourceAccessDetector"
      ],
      "metadata": {
        "id": "smQBbreNJSGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResourceAccessDetector(AnomalyDetector):\n",
        "    def __init__(self, contamination=0.1):\n",
        "        super().__init__()\n",
        "        self.model = IsolationForest(contamination=contamination, random_state=42)\n",
        "        self.scaler = StandardScaler()\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def _prepare_features(self, X):\n",
        "        \"\"\"Подготовка признаков для анализа доступа к ресурсам\"\"\"\n",
        "        features = np.column_stack([\n",
        "            X['is_admin'],\n",
        "            X['unique_pcs'],\n",
        "            X['device_usage_ratio'],\n",
        "            X['O'], X['C'], X['E'], X['A'], X['N']  # Психометрические характеристики\n",
        "        ])\n",
        "        if not self.is_fitted:\n",
        "            self.is_fitted = True\n",
        "            return self.scaler.fit_transform(features)\n",
        "        return self.scaler.transform(features)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        features = self._prepare_features(X)\n",
        "        self.model.fit(features)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        features = self._prepare_features(X)\n",
        "        return self.model.predict(features)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        features = self._prepare_features(X)\n",
        "        scores = -self.model.score_samples(features)\n",
        "        return scores / np.max(scores) if len(scores) > 0 else scores"
      ],
      "metadata": {
        "id": "rcQZVK4FJPCo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TimeActivityDetector"
      ],
      "metadata": {
        "id": "S2LT6IcwJWo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeActivityDetector(AnomalyDetector):\n",
        "    def __init__(self, contamination=0.1):\n",
        "        super().__init__()\n",
        "        self.model = IsolationForest(contamination=contamination, random_state=42)\n",
        "\n",
        "    def _prepare_features(self, X):\n",
        "        \"\"\"Подготовка признаков для анализа временных паттернов\"\"\"\n",
        "        features = np.column_stack([\n",
        "            X['avg_logons_per_day'],\n",
        "            X['weekend_logon_ratio'],\n",
        "            X['after_hours_logon_ratio'],\n",
        "            X['avg_device_usage_per_day'],\n",
        "            X['device_usage_ratio']\n",
        "        ])\n",
        "        return features\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        features = self._prepare_features(X)\n",
        "        self.model.fit(features)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        features = self._prepare_features(X)\n",
        "        return self.model.predict(features)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        features = self._prepare_features(X)\n",
        "        scores = -self.model.score_samples(features)\n",
        "        return scores / np.max(scores) if len(scores) > 0 else scores"
      ],
      "metadata": {
        "id": "T7ubfnGwJT-D"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EnsembleDetector"
      ],
      "metadata": {
        "id": "6YS1eZs_JcoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleDetector(AnomalyDetector):\n",
        "    def __init__(self, detectors, weights=None):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        detectors : dict\n",
        "            Словарь детекторов {name: detector}\n",
        "        weights : dict, optional\n",
        "            Веса для каждого детектора {name: weight}\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.detectors = detectors\n",
        "        self.weights = weights or {name: 1.0 for name in detectors.keys()}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Обучение всех базовых детекторов\"\"\"\n",
        "        print(\"\\nОбучение базовых детекторов...\")\n",
        "        for name, detector in self.detectors.items():\n",
        "            print(f\"Обучение {name}...\")\n",
        "            detector.fit(X)\n",
        "        return self\n",
        "\n",
        "    def _get_base_predictions(self, X):\n",
        "        \"\"\"Получение предсказаний от всех базовых детекторов\"\"\"\n",
        "        predictions = {}\n",
        "        for name, detector in self.detectors.items():\n",
        "            pred = detector.predict_proba(X)\n",
        "            predictions[name] = pred * self.weights[name]\n",
        "        return predictions\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Вероятностные оценки аномальности\"\"\"\n",
        "        base_predictions = self._get_base_predictions(X)\n",
        "\n",
        "        # Взвешенное среднее всех предсказаний\n",
        "        weighted_sum = np.zeros(len(X))\n",
        "        total_weight = 0\n",
        "\n",
        "        for name, pred in base_predictions.items():\n",
        "            weight = self.weights[name]\n",
        "            weighted_sum += pred * weight\n",
        "            total_weight += weight\n",
        "\n",
        "        return weighted_sum / total_weight if total_weight > 0 else weighted_sum\n",
        "\n",
        "    def predict(self, X, threshold=0.8):\n",
        "        \"\"\"\n",
        "        Предсказание аномальности\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pandas.DataFrame\n",
        "            Входные данные\n",
        "        threshold : float, default=0.8\n",
        "            Порог для определения аномалии\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            Массив меток: -1 - аномалия, 1 - норма\n",
        "        \"\"\"\n",
        "        probas = self.predict_proba(X)\n",
        "        return np.where(probas > threshold, -1, 1)\n",
        "\n",
        "    def update_weights(self, new_weights):\n",
        "        \"\"\"Обновление весов базовых детекторов\"\"\"\n",
        "        self.weights.update(new_weights)"
      ],
      "metadata": {
        "id": "TikLDwnOJYUy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "uXPnl3XSJxvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, base_path):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        base_path : str\n",
        "            Путь к директории с данными\n",
        "        \"\"\"\n",
        "        self.base_path = base_path\n",
        "        self.datasets = ['r1', 'r2', 'r3.1']  # Добавляем r1 в список поддерживаемых датасетов\n",
        "        self.current_dataset = None\n",
        "        self.logon_data = None\n",
        "        self.device_data = None\n",
        "        self.http_data = None\n",
        "        self.email_data = None\n",
        "        self.file_data = None\n",
        "        self.ldap_data = None\n",
        "        self.psychometric_data = None\n",
        "        self.features = None\n",
        "        self.insiders_data = None\n",
        "\n",
        "    def load_insiders_info(self):\n",
        "        \"\"\"Загрузка информации об инсайдерах\"\"\"\n",
        "        insiders_data = pd.DataFrame({\n",
        "            'dataset': ['r2', 'r3.1', 'r2'],\n",
        "            'scenario': [1, 1, 2],\n",
        "            'user': ['ONS0995', 'CSF0929', 'CCH0959'],\n",
        "            'start': pd.to_datetime(['3/6/2010 1:41:56', '07/01/2010 01:24:58', '08/02/2010 10:34:31']),\n",
        "            'end': pd.to_datetime(['3/20/2010 8:10:12', '07/16/2010 06:52:00', '09/30/2010 15:04:03']),\n",
        "        })\n",
        "        self.insiders_data = insiders_data\n",
        "        return insiders_data\n",
        "\n",
        "    def load_data(self, dataset='r3.1'):\n",
        "        \"\"\"Загрузка данных из указанного набора\"\"\"\n",
        "        if dataset not in self.datasets:\n",
        "            raise ValueError(f\"Dataset {dataset} not supported. Available datasets: {self.datasets}\")\n",
        "\n",
        "        self.current_dataset = dataset\n",
        "        dataset_path = os.path.join(self.base_path, dataset)\n",
        "\n",
        "        # Загрузка информации об инсайдерах\n",
        "        self.load_insiders_info()\n",
        "\n",
        "        # Загрузка основных данных\n",
        "        print(f\"Loading data from {dataset}...\")\n",
        "\n",
        "        # Логи входа\n",
        "        print(\"Loading logon data...\")\n",
        "        try:\n",
        "            self.logon_data = pd.read_csv(os.path.join(dataset_path, 'logon.csv'))\n",
        "            # Проверяем и преобразуем столбец с датой\n",
        "            date_column = next((col for col in ['date', 'timestamp'] if col in self.logon_data.columns), None)\n",
        "            if date_column:\n",
        "                self.logon_data[date_column] = pd.to_datetime(self.logon_data[date_column])\n",
        "                if date_column != 'date':\n",
        "                    self.logon_data = self.logon_data.rename(columns={date_column: 'date'})\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading logon data: {e}\")\n",
        "            self.logon_data = pd.DataFrame(columns=['user', 'pc', 'date', 'activity'])\n",
        "\n",
        "        # Данные устройств\n",
        "        print(\"Loading device data...\")\n",
        "        try:\n",
        "            self.device_data = pd.read_csv(os.path.join(dataset_path, 'device.csv'))\n",
        "            date_column = next((col for col in ['date', 'timestamp'] if col in self.device_data.columns), None)\n",
        "            if date_column:\n",
        "                self.device_data[date_column] = pd.to_datetime(self.device_data[date_column])\n",
        "                if date_column != 'date':\n",
        "                    self.device_data = self.device_data.rename(columns={date_column: 'date'})\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading device data: {e}\")\n",
        "            self.device_data = pd.DataFrame(columns=['user', 'pc', 'date', 'activity'])\n",
        "\n",
        "        # HTTP данные\n",
        "        print(\"Loading HTTP data...\")\n",
        "        try:\n",
        "            self.http_data = pd.read_csv(os.path.join(dataset_path, 'http.csv'))\n",
        "            # Проверяем наличие столбца user или id\n",
        "            if 'id' in self.http_data.columns and 'user' not in self.http_data.columns:\n",
        "                self.http_data = self.http_data.rename(columns={'id': 'user'})\n",
        "\n",
        "            # Проверяем и преобразуем столбец с датой\n",
        "            date_column = next((col for col in ['date', 'timestamp'] if col in self.http_data.columns), None)\n",
        "            if date_column:\n",
        "                self.http_data[date_column] = pd.to_datetime(self.http_data[date_column])\n",
        "                if date_column != 'date':\n",
        "                    self.http_data = self.http_data.rename(columns={date_column: 'date'})\n",
        "\n",
        "            # Если нет столбца url, но есть website\n",
        "            if 'website' in self.http_data.columns and 'url' not in self.http_data.columns:\n",
        "                self.http_data = self.http_data.rename(columns={'website': 'url'})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading HTTP data: {e}\")\n",
        "            self.http_data = pd.DataFrame(columns=['user', 'pc', 'date', 'url'])\n",
        "\n",
        "        # Email данные\n",
        "        print(\"Loading email data...\")\n",
        "        try:\n",
        "            self.email_data = pd.read_csv(os.path.join(dataset_path, 'email.csv'))\n",
        "            date_column = next((col for col in ['date', 'timestamp'] if col in self.email_data.columns), None)\n",
        "            if date_column:\n",
        "                self.email_data[date_column] = pd.to_datetime(self.email_data[date_column])\n",
        "                if date_column != 'date':\n",
        "                    self.email_data = self.email_data.rename(columns={date_column: 'date'})\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading email data: {e}\")\n",
        "            self.email_data = pd.DataFrame(columns=['from', 'to', 'date'])\n",
        "\n",
        "        # Файловые операции (только для r3.1)\n",
        "        if dataset == 'r3.1':\n",
        "            print(\"Loading file data...\")\n",
        "            try:\n",
        "                self.file_data = pd.read_csv(os.path.join(dataset_path, 'file.csv'))\n",
        "                # Проверяем и преобразуем столбец с датой\n",
        "                date_column = 'date' if 'date' in self.file_data.columns else 'timestamp'\n",
        "                self.file_data[date_column] = pd.to_datetime(self.file_data[date_column])\n",
        "                if date_column != 'date':\n",
        "                    self.file_data = self.file_data.rename(columns={date_column: 'date'})\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading file data: {e}\")\n",
        "                self.file_data = pd.DataFrame(columns=['user', 'pc', 'date', 'filename'])\n",
        "        else:\n",
        "            self.file_data = pd.DataFrame(columns=['user', 'pc', 'date', 'filename'])\n",
        "\n",
        "        # Психометрические данные\n",
        "        try:\n",
        "            print(\"Loading psychometric data...\")\n",
        "            self.psychometric_data = pd.read_csv(os.path.join(dataset_path, 'psychometric.csv'))\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading psychometric data: {e}\")\n",
        "            self.psychometric_data = pd.DataFrame(columns=['user_id', 'O', 'C', 'E', 'A', 'N'])\n",
        "\n",
        "        # LDAP данные\n",
        "        print(\"Loading LDAP data...\")\n",
        "        try:\n",
        "            ldap_files = glob.glob(os.path.join(dataset_path, 'LDAP', '*.csv'))\n",
        "            if ldap_files:\n",
        "                ldap_dfs = []\n",
        "                for file in ldap_files:\n",
        "                    try:\n",
        "                        df = pd.read_csv(file)\n",
        "                        month = os.path.basename(file).split('.')[0]\n",
        "                        df['month'] = month\n",
        "                        ldap_dfs.append(df)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading LDAP file {file}: {e}\")\n",
        "                if ldap_dfs:\n",
        "                    self.ldap_data = pd.concat(ldap_dfs, ignore_index=True)\n",
        "                else:\n",
        "                    self.ldap_data = pd.DataFrame(columns=['user_id', 'role', 'month'])\n",
        "            else:\n",
        "                print(\"No LDAP files found\")\n",
        "                self.ldap_data = pd.DataFrame(columns=['user_id', 'role', 'month'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing LDAP data: {e}\")\n",
        "            self.ldap_data = pd.DataFrame(columns=['user_id', 'role', 'month'])\n",
        "\n",
        "    def is_weekend(self, date):\n",
        "        \"\"\"Проверка является ли день выходным\"\"\"\n",
        "        return date.weekday() >= 5\n",
        "\n",
        "    def is_after_hours(self, date):\n",
        "        \"\"\"Проверка является ли время нерабочим\"\"\"\n",
        "        hour = date.hour\n",
        "        return hour < 7 or hour > 18\n",
        "\n",
        "    def prepare_features(self):\n",
        "        \"\"\"Подготовка признаков для обучения\"\"\"\n",
        "        features = []\n",
        "\n",
        "        # Получаем список всех пользователей\n",
        "        users = pd.unique(self.logon_data['user'])\n",
        "\n",
        "        for user in users:\n",
        "            # Фильтруем данные пользователя\n",
        "            user_logons = self.logon_data[self.logon_data['user'] == user]\n",
        "            user_devices = self.device_data[self.device_data['user'] == user]\n",
        "            user_http = self.http_data[self.http_data['user'] == user] if 'user' in self.http_data.columns else pd.DataFrame()\n",
        "            user_emails = self.email_data[\n",
        "                (self.email_data['from'] == user) |\n",
        "                (self.email_data['to'].str.contains(user, na=False))\n",
        "            ] if not self.email_data.empty else pd.DataFrame()\n",
        "\n",
        "            # Базовые характеристики\n",
        "            total_days = (user_logons['date'].max() - user_logons['date'].min()).days + 1\n",
        "            if total_days == 0:  # Если все события в один день\n",
        "                total_days = 1\n",
        "\n",
        "            # Характеристики входов\n",
        "            logon_features = {\n",
        "                'avg_logons_per_day': len(user_logons) / total_days,\n",
        "                'weekend_logon_ratio': user_logons[user_logons['date'].apply(self.is_weekend)].shape[0] / len(user_logons) if len(user_logons) > 0 else 0,\n",
        "                'after_hours_logon_ratio': user_logons[user_logons['date'].apply(self.is_after_hours)].shape[0] / len(user_logons) if len(user_logons) > 0 else 0,\n",
        "                'unique_pcs': user_logons['pc'].nunique(),\n",
        "            }\n",
        "\n",
        "            # Характеристики устройств\n",
        "            device_features = {\n",
        "                'avg_device_usage_per_day': len(user_devices) / total_days,\n",
        "                'device_usage_ratio': len(user_devices[user_devices['activity'] == 'Connect']) / total_days if total_days > 0 else 0,\n",
        "            }\n",
        "\n",
        "            # HTTP характеристики\n",
        "            http_features = {\n",
        "                'avg_http_requests_per_day': len(user_http) / total_days if not user_http.empty else 0,\n",
        "                'unique_domains': user_http['url'].apply(lambda x: x.split('/')[0]).nunique() if not user_http.empty and 'url' in user_http.columns else 0,\n",
        "            }\n",
        "\n",
        "            # Email характеристики\n",
        "            email_features = {\n",
        "                'avg_emails_sent_per_day': len(user_emails[user_emails['from'] == user]) / total_days if not user_emails.empty else 0,\n",
        "                'avg_emails_received_per_day': len(user_emails[user_emails['to'].str.contains(user, na=False)]) / total_days if not user_emails.empty else 0,\n",
        "                'unique_email_contacts': pd.concat([\n",
        "                    user_emails['to'].str.split(';').explode(),\n",
        "                    user_emails['from']\n",
        "                ]).nunique() if not user_emails.empty else 0,\n",
        "            }\n",
        "\n",
        "            # Дополнительные характеристики для r3.1\n",
        "            if self.current_dataset == 'r3.1' and not self.file_data.empty:\n",
        "                user_files = self.file_data[self.file_data['user'] == user]\n",
        "                file_features = {\n",
        "                    'avg_file_copies_per_day': len(user_files) / total_days,\n",
        "                    'unique_file_types': user_files['filename'].apply(lambda x: x.split('.')[-1] if '.' in x else '').nunique(),\n",
        "                }\n",
        "            else:\n",
        "                file_features = {\n",
        "                    'avg_file_copies_per_day': 0,\n",
        "                    'unique_file_types': 0,\n",
        "                }\n",
        "\n",
        "            # Психометрические характеристики\n",
        "            user_psycho = self.psychometric_data[self.psychometric_data['user_id'] == user]\n",
        "            if not user_psycho.empty:\n",
        "                psycho_features = user_psycho.iloc[0][['O', 'C', 'E', 'A', 'N']].to_dict()\n",
        "            else:\n",
        "                psycho_features = {\n",
        "                    'O': 0, 'C': 0, 'E': 0, 'A': 0, 'N': 0\n",
        "                }\n",
        "\n",
        "            # LDAP характеристики\n",
        "            user_ldap = self.ldap_data[self.ldap_data['user_id'] == user]\n",
        "            ldap_features = {\n",
        "                'is_admin': 1 if not user_ldap.empty and user_ldap.iloc[-1]['role'] == 'ITAdmin' else 0,\n",
        "            }\n",
        "\n",
        "            # Объединяем все характеристики\n",
        "            user_features = {\n",
        "                'user_id': user,\n",
        "                **logon_features,\n",
        "                **device_features,\n",
        "                **http_features,\n",
        "                **email_features,\n",
        "                **file_features,\n",
        "                **psycho_features,\n",
        "                **ldap_features,\n",
        "            }\n",
        "\n",
        "            features.append(user_features)\n",
        "\n",
        "        # Создаем DataFrame\n",
        "        self.features = pd.DataFrame(features)\n",
        "\n",
        "        # Нормализация числовых признаков\n",
        "        numeric_columns = self.features.select_dtypes(include=[np.number]).columns\n",
        "        numeric_columns = numeric_columns.drop(['user_id', 'is_admin']) if 'user_id' in numeric_columns else numeric_columns\n",
        "\n",
        "        if not numeric_columns.empty:\n",
        "            scaler = StandardScaler()\n",
        "            self.features[numeric_columns] = scaler.fit_transform(self.features[numeric_columns])\n",
        "\n",
        "        return self.features\n",
        "\n",
        "    def get_user_timeline(self, user):\n",
        "        \"\"\"Получение временной линии событий пользователя\"\"\"\n",
        "        timeline = []\n",
        "\n",
        "        # Добавляем логины\n",
        "        logons = self.logon_data[self.logon_data['user'] == user]\n",
        "        for _, row in logons.iterrows():\n",
        "            timeline.append({\n",
        "                'date': row['date'],\n",
        "                'type': 'logon',\n",
        "                'details': f\"{row['activity']} on {row['pc']}\"\n",
        "            })\n",
        "\n",
        "        # Добавляем использование устройств\n",
        "        devices = self.device_data[self.device_data['user'] == user]\n",
        "        for _, row in devices.iterrows():\n",
        "            timeline.append({\n",
        "                'date': row['date'],\n",
        "                'type': 'device',\n",
        "                'details': f\"{row['activity']} on {row['pc']}\"\n",
        "            })\n",
        "\n",
        "        # Добавляем HTTP запросы\n",
        "        http = self.http_data[self.http_data['user'] == user]\n",
        "        for _, row in http.iterrows():\n",
        "            timeline.append({\n",
        "                'date': row['date'],\n",
        "                'type': 'http',\n",
        "                'details': f\"Visited {row['url']}\"\n",
        "            })\n",
        "\n",
        "        # Добавляем email активность\n",
        "        emails_sent = self.email_data[self.email_data['from'] == user]\n",
        "        emails_received = self.email_data[self.email_data['to'].str.contains(user, na=False)]\n",
        "\n",
        "        for _, row in emails_sent.iterrows():\n",
        "            timeline.append({\n",
        "                'date': row['date'],\n",
        "                'type': 'email',\n",
        "                'details': f\"Sent email to {row['to']}\"\n",
        "            })\n",
        "\n",
        "        for _, row in emails_received.iterrows():\n",
        "            timeline.append({\n",
        "                'date': row['date'],\n",
        "                'type': 'email',\n",
        "                'details': f\"Received email from {row['from']}\"\n",
        "            })\n",
        "\n",
        "        # Добавляем файловые операции для r3.1\n",
        "        if self.current_dataset == 'r3.1' and self.file_data is not None:\n",
        "            files = self.file_data[self.file_data['user'] == user]\n",
        "            for _, row in files.iterrows():\n",
        "                timeline.append({\n",
        "                    'date': row['date'],\n",
        "                    'type': 'file',\n",
        "                    'details': f\"Copied file {row['filename']} on {row['pc']}\"\n",
        "                })\n",
        "\n",
        "        # Сортируем по времени\n",
        "        timeline = pd.DataFrame(timeline)\n",
        "        timeline = timeline.sort_values('date')\n",
        "\n",
        "        return timeline"
      ],
      "metadata": {
        "id": "Gz_f9LDlJfX3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "MxRgFq8dJ-dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def create_anomaly_detection_system():\n",
        "    \"\"\"Создание системы детектирования аномалий\"\"\"\n",
        "    print(\"\\n[1/4] Инициализация детекторов...\")\n",
        "\n",
        "    # Создаем базовые детекторы с повышенной чувствительностью\n",
        "    detectors = {\n",
        "        'time_activity': TimeActivityDetector(contamination=0.05),\n",
        "        'data_frequency': DataFrequencyDetector(window_size=3600, contamination=0.05),\n",
        "        'data_volume': DataVolumeDetector(window_size=3600, contamination=0.05),\n",
        "        'resource_access': ResourceAccessDetector(contamination=0.05),\n",
        "        'file_activity': FileActivityDetector(contamination=0.05)\n",
        "    }\n",
        "\n",
        "    print(\"Настройка весов детекторов...\")\n",
        "    # Создаем веса для каждого детектора\n",
        "    weights = {\n",
        "        'time_activity': 1.5,    # Повышенный вес для временных паттернов\n",
        "        'data_frequency': 1.2,   # Повышенный вес для частоты обращений\n",
        "        'data_volume': 1.2,      # Повышенный вес для объема данных\n",
        "        'resource_access': 1.5,  # Повышенный вес для доступа к ресурсам\n",
        "        'file_activity': 1.5     # Повышенный вес для файловых операций\n",
        "    }\n",
        "\n",
        "    # Создаем ансамбль\n",
        "    print(\"Создание ансамбля детекторов...\")\n",
        "    ensemble = EnsembleDetector(detectors, weights)\n",
        "    return ensemble\n",
        "\n",
        "def evaluate_detection(features, predictions, probabilities, loader, dataset):\n",
        "    \"\"\"Оценка качества обнаружения инсайдеров\"\"\"\n",
        "    print(\"\\n[3/4] Оценка результатов детектирования...\")\n",
        "\n",
        "    print(\"Получение списка реальных инсайдеров...\")\n",
        "    # Получаем список реальных инсайдеров для текущего датасета\n",
        "    real_insiders = loader.insiders_data[loader.insiders_data['dataset'] == dataset]['user'].tolist()\n",
        "\n",
        "    print(\"Подготовка массива истинных меток...\")\n",
        "    # Создаем массив истинных меток\n",
        "    y_true = np.zeros(len(features))\n",
        "    for idx, user in tqdm(enumerate(features['user_id']),\n",
        "                         desc=\"Разметка пользователей\",\n",
        "                         total=len(features)):\n",
        "        if user in real_insiders:\n",
        "            y_true[idx] = 1\n",
        "\n",
        "    print(\"Вычисление метрик качества...\")\n",
        "    # Преобразуем предсказания из [-1, 1] в [0, 1]\n",
        "    y_pred = (predictions == -1).astype(int)\n",
        "\n",
        "    # Вычисляем метрики\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    print(\"\\nМетрики качества обнаружения:\")\n",
        "    print(f\"Precision (точность): {precision:.3f}\")\n",
        "    print(f\"Recall (полнота): {recall:.3f}\")\n",
        "    print(f\"F1-score: {f1:.3f}\")\n",
        "\n",
        "    # Анализ результатов\n",
        "    print(\"\\nАнализ результатов обнаружения...\")\n",
        "    detected_insiders = []\n",
        "    missed_insiders = []\n",
        "    false_positives = []\n",
        "\n",
        "    for idx, (user, prob) in tqdm(enumerate(zip(features['user_id'], probabilities)),\n",
        "                                 desc=\"Анализ предсказаний\",\n",
        "                                 total=len(features)):\n",
        "        if y_pred[idx] == 1:  # Если обнаружена аномалия\n",
        "            if user in real_insiders:\n",
        "                detected_insiders.append((user, prob))\n",
        "            else:\n",
        "                false_positives.append((user, prob))\n",
        "        elif user in real_insiders:\n",
        "            missed_insiders.append((user, prob))\n",
        "\n",
        "    # Вывод результатов\n",
        "    print(\"\\nРезультаты анализа:\")\n",
        "    print(f\"Всего пользователей проанализировано: {len(features)}\")\n",
        "    print(f\"Обнаружено потенциальных аномалий: {len(detected_insiders) + len(false_positives)}\")\n",
        "    print(f\"Правильно обнаружено инсайдеров: {len(detected_insiders)}\")\n",
        "    print(f\"Ложных срабатываний: {len(false_positives)}\")\n",
        "    print(f\"Пропущено инсайдеров: {len(missed_insiders)}\")\n",
        "\n",
        "    print(\"\\nОбнаруженные инсайдеры:\")\n",
        "    for user, prob in detected_insiders:\n",
        "        insider_info = loader.insiders_data[\n",
        "            (loader.insiders_data['dataset'] == dataset) &\n",
        "            (loader.insiders_data['user'] == user)\n",
        "        ].iloc[0]\n",
        "        print(f\"- {user} (вероятность: {prob:.3f})\")\n",
        "        print(f\"  Сценарий: {insider_info['scenario']}\")\n",
        "        print(f\"  Период активности: {insider_info['start']} - {insider_info['end']}\")\n",
        "\n",
        "    if missed_insiders:\n",
        "        print(\"\\nПропущенные инсайдеры:\")\n",
        "        for user, prob in missed_insiders:\n",
        "            print(f\"- {user} (вероятность: {prob:.3f})\")\n",
        "\n",
        "    print(\"\\nЛожные срабатывания (топ-5 по вероятности):\")\n",
        "    for user, prob in sorted(false_positives, key=lambda x: x[1], reverse=True)[:5]:\n",
        "        print(f\"- {user} (вероятность: {prob:.3f})\")\n",
        "\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "    print(\"\\n=== Запуск системы обнаружения инсайдеров ===\")\n",
        "\n",
        "    # Анализируем все доступные датасеты\n",
        "    datasets = ['r1', 'r2', 'r3.1']\n",
        "\n",
        "    for dataset in datasets:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Анализ датасета {dataset}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Загружаем данные\n",
        "        print(f\"\\n[1/5] Загрузка данных из датасета {dataset}...\")\n",
        "        loader = DataLoader('dataset')\n",
        "        loader.load_data(dataset)\n",
        "\n",
        "        # Подготавливаем признаки\n",
        "        print(\"\\n[2/5] Подготовка признаков...\")\n",
        "        features = loader.prepare_features()\n",
        "        print(f\"Извлечено {features.shape[1]} признаков для {features.shape[0]} пользователей\")\n",
        "\n",
        "        # Используем только признаки без меток\n",
        "        X = features.drop(['user_id'], axis=1)\n",
        "\n",
        "        # Создаем систему детектирования\n",
        "        print(\"\\n[3/5] Создание системы детектирования...\")\n",
        "        system = create_anomaly_detection_system()\n",
        "\n",
        "        # Обучаем систему\n",
        "        print(\"\\n[4/5] Обучение моделей...\")\n",
        "        system.fit(X)\n",
        "\n",
        "        # Получаем предсказания для всех пользователей\n",
        "        print(\"\\n[5/5] Анализ поведения пользователей...\")\n",
        "        predictions = system.predict(X)\n",
        "        probabilities = system.predict_proba(X)\n",
        "\n",
        "        # Оцениваем качество обнаружения\n",
        "        evaluate_detection(features, predictions, probabilities, loader, dataset)\n",
        "\n",
        "        # Анализируем пользователей с высоким риском\n",
        "        print(\"\\nПодробный анализ пользователей высокого риска...\")\n",
        "        high_risk_mask = probabilities > 0.8\n",
        "        high_risk_users = features.loc[high_risk_mask, 'user_id']\n",
        "\n",
        "        print(f\"\\nНайдено {len(high_risk_users)} пользователей с высоким риском (p > 0.8)\")\n",
        "\n",
        "        for user in tqdm(high_risk_users, desc=\"Анализ пользователей высокого риска\"):\n",
        "            print(f\"\\nАнализ пользователя {user}:\")\n",
        "            # Получаем временную линию событий пользователя\n",
        "            timeline = loader.get_user_timeline(user)\n",
        "            print(f\"Всего событий: {len(timeline)}\")\n",
        "            if not timeline.empty:\n",
        "                print(\"Типы событий:\")\n",
        "                print(timeline['type'].value_counts())\n",
        "\n",
        "                # Показываем примеры подозрительных действий\n",
        "                print(\"\\nПримеры последних действий:\")\n",
        "                print(timeline.tail().to_string())\n",
        "\n",
        "                # Если это реальный инсайдер, показываем дополнительную информацию\n",
        "                insider_info = loader.insiders_data[\n",
        "                    (loader.insiders_data['dataset'] == dataset) &\n",
        "                    (loader.insiders_data['user'] == user)\n",
        "                ]\n",
        "                if not insider_info.empty:\n",
        "                    info = insider_info.iloc[0]\n",
        "                    print(\"\\nПОДТВЕРЖДЕННЫЙ ИНСАЙДЕР!\")\n",
        "                    print(f\"Сценарий: {info['scenario']}\")\n",
        "                    print(f\"Период активности: {info['start']} - {info['end']}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"\\n=== Анализ всех датасетов завершен за {execution_time:.2f} секунд ===\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "JlQ545jyJ7qW",
        "outputId": "67f85305-9b4f-454c-e3b2-72f8862357d0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Запуск системы обнаружения инсайдеров ===\n",
            "\n",
            "==================================================\n",
            "Анализ датасета r1\n",
            "==================================================\n",
            "\n",
            "[1/5] Загрузка данных из датасета r1...\n",
            "Loading data from r1...\n",
            "Loading logon data...\n",
            "Error loading logon data: [Errno 2] No such file or directory: 'dataset/r1/logon.csv'\n",
            "Loading device data...\n",
            "Error loading device data: [Errno 2] No such file or directory: 'dataset/r1/device.csv'\n",
            "Loading HTTP data...\n",
            "Error loading HTTP data: [Errno 2] No such file or directory: 'dataset/r1/http.csv'\n",
            "Loading email data...\n",
            "Error loading email data: [Errno 2] No such file or directory: 'dataset/r1/email.csv'\n",
            "Loading psychometric data...\n",
            "Error loading psychometric data: [Errno 2] No such file or directory: 'dataset/r1/psychometric.csv'\n",
            "Loading LDAP data...\n",
            "No LDAP files found\n",
            "\n",
            "[2/5] Подготовка признаков...\n",
            "Извлечено 0 признаков для 0 пользователей\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['user_id'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-17f9e9dbc0ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-17f9e9dbc0ef>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# Используем только признаки без меток\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Создаем систему детектирования\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['user_id'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sXeH7jo8J-BK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}